{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb64bd0a-5525-47e5-ba0c-a68848080094",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Introduction\n",
    "\n",
    "This notebook is apart of the DSA Databricks Blog Post here: <link here>\n",
    "\n",
    "This is the final notebook setting up a genie space with fake patient data generated by the python library faker.  \n",
    "\n",
    "Recommended Compute Type: Classic Compute\n",
    "\n",
    "Recommended Runtime: 16.4 ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4407f024-bac8-4b9c-b14d-0a65b473ba18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca862a27-d8bf-4c44-932f-8d7ffe02cc3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faker\n  Downloading faker-37.3.0-py3-none-any.whl.metadata (15 kB)\nCollecting tzdata (from faker)\n  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\nDownloading faker-37.3.0-py3-none-any.whl (1.9 MB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/1.9 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.5/1.9 MB\u001B[0m \u001B[31m13.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m28.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hDownloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/347.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m347.8/347.8 kB\u001B[0m \u001B[31m28.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: tzdata, faker\nSuccessfully installed faker-37.3.0 tzdata-2025.2\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install faker\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f304e4bf-0c65-40cc-aabf-d58449c64df7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from config import volume_label, volume_name, catalog, schema, model_name, model_endpoint_name, embedding_table_name, embedding_table_name_index, registered_model_name, vector_search_endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c306fb24-2b12-4905-839b-968690768232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3659528e-9a97-407c-974a-88ce102e875c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(f\"CREATE CATALOG IF NOT EXISTS {catalog}\")\n",
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{schema}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {catalog}.{schema}.patient_visits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80054501-48c7-4ee4-8dbf-2ed63928937a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fake = Faker()\n",
    "\n",
    "# Define fixed options\n",
    "patients = [fake.unique.first_name() + \" \" + fake.unique.last_name() for _ in range(15)]\n",
    "insurance_providers = [\"BlueCross\", \"Aetna\", \"UnitedHealth\", \"Insurance Company 1\", \"Christus\"]\n",
    "insurance_types = [\"HMO\", \"PPO\", \"EPO\"]\n",
    "reasons_for_visit = [\n",
    "    \"Routine Checkup\", \"Flu Symptoms\", \"Injury\", \"Chronic Condition\", \n",
    "    \"Follow-up\", \"Prescription Refill\", \"Surgery\", \"Physical Therapy\"\n",
    "]\n",
    "cities = [\"LA\", \"Chicago\", \"NY\"]\n",
    "\n",
    "# Assign a random but fixed insurance provider and type to each patient\n",
    "patient_insurance = {patient: random.choice(insurance_providers) for patient in patients}\n",
    "patient_insurance_type = {patient: random.choice(insurance_types) for patient in patients}\n",
    "\n",
    "# Generate patient visit data\n",
    "data = []\n",
    "for _ in range(300):\n",
    "    patient = random.choice(patients)\n",
    "    first_name, last_name = patient.split(\" \")\n",
    "    insurance_provider = patient_insurance[patient]\n",
    "    insurance_type = patient_insurance_type[patient]\n",
    "    policy_number = fake.uuid4() if random.random() > 0.2 else None  # 80% chance to have a policy number\n",
    "    email = fake.email()\n",
    "    city = random.choice(cities)\n",
    "    practice_id = fake.random_int(min=1000, max=9999)\n",
    "    doctor_notes = fake.sentence()\n",
    "    reason_for_visit = random.choice(reasons_for_visit)\n",
    "\n",
    "    data.append([\n",
    "        first_name, last_name, insurance_provider, insurance_type, policy_number,\n",
    "        email, city, practice_id, doctor_notes, reason_for_visit\n",
    "    ])\n",
    "\n",
    "# Create DataFrame\n",
    "columns = [\n",
    "    \"first_name\", \"last_name\", \"insurance_provider_name\", \"insurance_type\",\n",
    "    \"insurance_policy_number\", \"email\", \"city\", \"practice_visited_practice_id\",\n",
    "    \"doctor_notes\", \"reason_for_visit\"\n",
    "]\n",
    "patients_visits_df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Convert to Spark DataFrame\n",
    "patients_visits_spark_df = spark.createDataFrame(patients_visits_df)\n",
    "\n",
    "# Save DataFrame to specified catalog and schema\n",
    "patients_visits_spark_df.write.saveAsTable(f\"{catalog}.{schema}.patient_visits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f00b415b-aec4-4d8d-a0a6-b67c166265ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Practice Location Table \n",
    "\n",
    "import random\n",
    "\n",
    "# Define possible values\n",
    "cities = [\"LA\", \"Chicago\", \"NY\"]\n",
    "insurance_providers = [\"BlueCross\", \"Aetna\", \"UnitedHealth\"]\n",
    "insurance_plan_types = [\"HMO\", \"PPO\", \"EPO\"]\n",
    "network_statuses = [\"In Network\", \"Out of Network\"]\n",
    "\n",
    "# Generate sample data\n",
    "num_entries = 50\n",
    "data = {\n",
    "    \"practice_name\": [f\"Medical Center {i}\" for i in range(1, num_entries + 1)],\n",
    "    \"city\": [random.choice(cities) for _ in range(num_entries)],\n",
    "    \"contact\": [f\"(555) 555-12{str(i).zfill(2)}\" for i in range(num_entries)],\n",
    "    \"insurance_id\": [f\"INS-{random.randint(1000, 9999)}\" for _ in range(num_entries)],\n",
    "    \"insurance_company\": [random.choice(insurance_providers) for _ in range(num_entries)],\n",
    "    \"insurance_plan_type\": [random.choice(insurance_plan_types) for _ in range(num_entries)],\n",
    "    \"network_status\": [random.choice(network_statuses) for _ in range(num_entries)],\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "practice_locations_df = spark.createDataFrame(pd.DataFrame(data))\n",
    "\n",
    "# Save DataFrame to specified catalog and schema\n",
    "practice_locations_df.write.saveAsTable(f\"{catalog}.{schema}.practice_locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07e5b797-72b6-4760-8147-7542c6b8ae1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Creating the Genie Space\n",
    "\n",
    "Unfortunately, there is no programatically way to create a genie space at this time. Go to the Genie Space section in the UI and create a new space pointing to the tables below\n",
    "\n",
    "You can follow the instructions here to understand how to set up a Genie Space: https://docs.databricks.com/aws/en/genie/set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c71bca36-a6b1-44cd-80ba-6415aceb232a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 1: austin_choi_demo_catalog.agents.patient_visits\nTable 2: austin_choi_demo_catalog.agents.practice_locations\n"
     ]
    }
   ],
   "source": [
    "print(f\"Table 1: {catalog}.{schema}.patient_visits\")\n",
    "print(f\"Table 2: {catalog}.{schema}.practice_locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "324f7bdd-02ba-42dd-8af4-e09601d09041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1315887243069239,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "03_Create Synthetic Data and the Genie Space",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
